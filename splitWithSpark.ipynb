{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "import os\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function chunk_input_file\n",
    "\n",
    "Defined a function to chunk the given input file based on the identifier column and writes each chunk to a separate .tsv file.\n",
    "\n",
    "#### Inputs:\n",
    "\n",
    "- **identifier_col**: Identifier column name.\n",
    "- **input_file_path**: Path of the input file.\n",
    "- **output_directory_path**: Path of the output directory.\n",
    "- **max_rows_per_chunk**: Maximum allowed rows per file.\n",
    "- **has_header**: If the output files have a header.\n",
    "- **parallel_partitions_number**: Number of parallel processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_input_file(identifier_col: Any , input_file_path: str, output_directory_path: str, max_rows_per_chunk: int, has_header: bool, parallel_partions_number: int):\n",
    "    \n",
    "    # A function to write the given chunk to .tsv file\n",
    "    # input chunk: Data Frame\n",
    "    # file_number: Integer\n",
    "    def write_chunk_to_file(chunk: DataFrame, file_number: int):\n",
    "        # for naming consistency\n",
    "        file_number += 1\n",
    "        # Output directory path for .csv method\n",
    "        created_dir = f\"{output_directory_path}/temporary_sample-part_{file_number}\"\n",
    "        # merge pieces created by spark prallel processors to a single .tsv \n",
    "        chunk.coalesce(1).write.option(\"delimiter\", \"\\t\").option(\"quote\",\"\\u0000\").option(\"escape\", \"\\u0000\").mode(\"overwrite\").csv(created_dir, header=has_header)\n",
    "        # finding .tsv file created by .csv method and rename it\n",
    "        output_file = f\"{created_dir}/part-00000-*\"\n",
    "        output_path = f\"{output_directory_path}/sample-part_{file_number}.tsv\"\n",
    "        # move the output file to correct path \n",
    "        os.system(f\"mv {output_file} {output_path}\")\n",
    "        # remove the old directory\n",
    "        os.system(f\"rm -r {created_dir}\")\n",
    "    \n",
    "    # create the output directory if it doesnt exists\n",
    "    if not os.path.exists(output_directory_path):\n",
    "        os.makedirs(output_directory_path)\n",
    "    \n",
    "    # Initialize a spark session to process the large .tsv file in parallel\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Chunk given tsv\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # create a spark data frame of the input file\n",
    "    spark_df = spark.read.option(\"delimiter\", \"\\t\").option(\"quote\",\"\\u0000\").option(\"escape\", \"\\u0000\").csv(input_file_path, header=True)\n",
    "    # repartition the spark dataframe to equal size partitions based on the \n",
    "    # parallel_partions_number variable (parallel processing each partition)\n",
    "    spark_df = spark_df.repartition(parallel_partions_number)\n",
    "    # Get unique identifier elements in the identifier column\n",
    "    distinct_identifiers = spark_df.select(identifier_col).distinct()\n",
    "\n",
    "    # set variables to store rows in chunk, its size, and split number\n",
    "    chunk = None\n",
    "    chunk_size = 0\n",
    "    file_number = 0\n",
    "    # Gather all distinct identifiers from distibuted data and iterate them\n",
    "    for identifier_element in distinct_identifiers.collect(): \n",
    "        # extract the identifier value\n",
    "        identifier_element_id = identifier_element[0]\n",
    "        # find rows with the same identifier value in each data partition in parallel and then aggregate the results\n",
    "        rows_with_identifier = spark_df.filter(spark_df[identifier_col] == identifier_element_id)\n",
    "        # count the number of rows calculated in the previous step\n",
    "        rows_with_identifier_count = rows_with_identifier.count()\n",
    "\n",
    "        # check if adding filtered data to the current chunk may exceeds the row limit\n",
    "        if chunk_size + rows_with_identifier_count > max_rows_per_chunk:\n",
    "            # if we have a current chunk and we are exceeding the limit \n",
    "            # write the chunk and clear it\n",
    "            # reset the size\n",
    "            # increase file split number\n",
    "            if chunk is not None:\n",
    "                write_chunk_to_file(chunk, file_number)\n",
    "                file_number += 1\n",
    "                chunk = None\n",
    "                chunk_size = 0\n",
    "\n",
    "        # if our chunk is empty fill it with filtered data, otherwise append the data to it\n",
    "        if chunk is None:\n",
    "            chunk = rows_with_identifier\n",
    "        else:\n",
    "            chunk = chunk.union(rows_with_identifier)\n",
    "        \n",
    "        # update the size variable\n",
    "        chunk_size += rows_with_identifier_count\n",
    "\n",
    "    # if at the end of the process there is a remained chunk (because it has not exceed the limit)\n",
    "    # write it to a separate chunk\n",
    "    if chunk is not None:\n",
    "        write_chunk_to_file(chunk, file_number)\n",
    "    # STOP the session\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the configurations and run the function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/03 18:15:46 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# set config values \n",
    "configs = {\n",
    "    \"identifier_col\": '\"filename\"' , \n",
    "    \"input_file_path\": \"shared/sample-data/sample.tsv\", \n",
    "    \"output_directory_path\": \"shared/output\", \n",
    "    \"max_rows_per_chunk\": 3, \n",
    "    \"has_header\": True, \n",
    "    \"parallel_partions_number\": 5\n",
    "}\n",
    "\n",
    "# run the function using config values as input parameters\n",
    "chunk_input_file(identifier_col= configs[\"identifier_col\"], \n",
    "                 input_file_path= configs[\"input_file_path\"], \n",
    "                 output_directory_path= configs[\"output_directory_path\"], \n",
    "                 max_rows_per_chunk= configs[\"max_rows_per_chunk\"], \n",
    "                 has_header= configs[\"has_header\"], \n",
    "                 parallel_partions_number= configs[\"parallel_partions_number\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "check_icv",
   "language": "python",
   "name": "check_icv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
